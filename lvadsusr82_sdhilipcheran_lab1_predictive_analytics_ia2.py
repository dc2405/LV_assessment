# -*- coding: utf-8 -*-
"""LVADSUSR82_SDHILIPCHERAN_LAB1_Predictive analytics_IA2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GYEv8QtNDhNM8X7ZqkFyFXfR5Yw_KRE1
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix

wine_data = pd.read_csv(/content/winequality-red.csv)

#a.i. Handling Missing Values
missing_values = wine_data.isnull().sum()
print("Missing Values:")
print(missing_values)

#Filling missing values with mean
wine_data.fillna(wine_data.mean(), inplace=True)

# ii. Identify and manage outliers in the data
outlier_detector = IsolationForest(contamination=0.05, random_state=42)
outlier_labels = outlier_detector.fit_predict(wine_data.drop('Quality', axis=1))
#Remove outliers
wine_data_cleaned = wine_data[outlier_labels == 1]

# b. Data Transformation
# Transforming the target variable
wine_data['Quality'] = wine_data['Quality'].apply(lambda x: 'Bad' if x < 6 else 'Good')

# c. Encoding and Balancing Data
encoder = LabelEncoder()
wine_data['Quality'] = encoder.fit_transform(wine_data['Quality'])

# d. Feature Selection and Data Cleaning
#I am using all the features for now.
#Data cleaning:

missing_values = wine_data.isnull().sum()
print("Missing Values:")
print(missing_values)


wine_data.fillna(wine_data.mean(), inplace=True)

#Outlier Detection and Management

from scipy import stats
z_scores = stats.zscore(wine_data)
abs_z_scores = np.abs(z_scores)
filtered_entries = (abs_z_scores < 3).all(axis=1)
wine_data = wine_data[filtered_entries]

#Now, the data is clean after handling missing values and outliers.

# Displaying cleaned dataset
print("Cleaned Dataset:")
print(wine_data.head())

# e. Data Splitting
X = wine_data.drop('Quality', axis=1)
y = wine_data['Quality']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# f. Model Development
# Implementing K-NN

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Implementing Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# g. Model Evaluation
# Evaluatng K-NN model
knn_pred = knn.predict(X_test)
knn_accuracy = accuracy_score(y_test, knn_pred)
knn_precision = precision_score(y_test, knn_pred)
knn_recall = recall_score(y_test, knn_pred)
print("K-NN Model Metrics:")
print("Accuracy:", knn_accuracy)
print("Precision:", knn_precision)
print("Recall:", knn_recall)

# Evaluating Random Forest model
rf_pred = rf.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)
rf_precision = precision_score(y_test, rf_pred)
rf_recall = recall_score(y_test, rf_pred)
print("\nRandom Forest Model Metrics:")
print("Accuracy:", rf_accuracy)
print("Precision:", rf_precision)
print("Recall:", rf_recall)

# Confusion Matrix
print("\nConfusion Matrix for K-NN:")
print(confusion_matrix(y_test, knn_pred))
print("\nConfusion Matrix for Random Forest:")
print(confusion_matrix(y_test, rf_pred))