# -*- coding: utf-8 -*-
"""LVDASUSR82_S DHILIP CHERAN_LAB 1_linear regression_IA1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g_KIctNS243kHs535vdJ2cXjl2eRH5Ag
"""

#S DHILIP CHERAN
#LAB 1 LINEAR REGRESSION
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

#Loading the dataset
data = pd.read_csv('/content/expenses.csv')

#1.5 TASKS:
#1: Handling Missing Values and Outliers
# Check for missing values
missing_values = data.isnull().sum()
print("Missing values:")
print(missing_values)

# Identify and manage outliers

# Identify and manage outliers using the Interquartile Range (IQR) method
def detect_outliers(df, features):
    outlier_indices = []
    for col in features:
        # 1st quartile (25%)
        Q1 = np.percentile(df[col], 25)
        # 3rd quartile (75%)
        Q3 = np.percentile(df[col], 75)
        # Interquartile range (IQR)
        IQR = Q3 - Q1
        # Outlier step
        outlier_step = 1.5 * IQR
        # Determine a list of indices of outliers for feature col
        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index
        # Append the found outlier indices for col to the list of outlier indices
        outlier_indices.extend(outlier_list_col)
    return list(set(outlier_indices))

# Identifying outliers from numerical features
outliers = detect_outliers(data, ['age', 'bmi', 'children', 'charges'])

# Removing the outliers
data_cleaned = data.drop(outliers, axis=0).reset_index(drop=True)

print("\nNumber of outliers detected:", len(outliers))
print("Dataset shape after removing outliers:", data_cleaned.shape)


#2: Encoding Categorical Data
encoder = OneHotEncoder(drop='first')
categorical_features = ['sex', 'smoker', 'region']
transformers = [('encoder', encoder, categorical_features)]
column_transformer = ColumnTransformer(transformers, remainder='passthrough')
data_encoded = column_transformer.fit_transform(data)

#3: Feature Selection and Data Cleaning
df = pd.read_csv("/content/expenses.csv")

# Data Cleaning
imputer = SimpleImputer(strategy='mean')
df_cleaned = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Handling Outliers
df_cleaned = df_cleaned[(df_cleaned['column_name'] >= lower_bound) & (df_cleaned['column_name'] <= upper_bound)]

# Feature Selection
X = df_cleaned.drop[('age','sex','bmi','children','smoker','region'), axis=1]
y = df_cleaned['charges']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

selector = SelectKBest(score_func=f_classif, k=5)
X_selected = selector.fit_transform(X_scaled, y)

selected_feature_names = X.columns[selector.get_support()]


#4: Data Splitting
X = data_encoded[:, :-1]  # Features
y = data_encoded[:, -1]   # Target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#5: Model Development and Training
#linear regression model
model = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler()),
    ('regressor', LinearRegression())
])


model.fit(X_train, y_train)

#6: Model Evaluation

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mse)

print("\nModel Evaluation:")
print("Mean Squared Error (MSE):", mse)
print("R-squared (R2):", r2)
print("Root Mean Squared Error (RMSE):", rmse)