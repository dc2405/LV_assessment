# -*- coding: utf-8 -*-
"""LVDASUSR82_S DHILIP CHERAN_LAB 2_classification_IA1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xYs1Hby0pzZeAidR-Hdm7atM8K9ODQy6
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

data = pd.read_csv('/content/booking.csv')

#1: Handling Missing Values and Outliers
missing_values = data.isnull().sum()
print("Missing values:")
print(missing_values)

missing_values = data.isnull().sum()
print("Missing values:")
print(missing_values)


# Identifying and managing outliers
def detect_outliers(df, features):
    outlier_indices = []
    for col in features:
        # 1st quartile (25%)
        Q1 = np.percentile(df[col], 25)
        # 3rd quartile (75%)
        Q3 = np.percentile(df[col], 75)
        # Interquartile range (IQR)
        IQR = Q3 - Q1
        # Outlier step
        outlier_step = 1.5 * IQR
        # Determine a list of indices of outliers for feature col
        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index
        # Append the found outlier indices for col to the list of outlier indices
        outlier_indices.extend(outlier_list_col)
    return list(set(outlier_indices))

# Getting outliers from numerical features
outliers = detect_outliers(data, ['age', 'bmi', 'children', 'charges'])

# Removing outliers
data_cleaned = data.drop(outliers, axis=0).reset_index(drop=True)

print("\nNumber of outliers detected:", len(outliers))
print("Dataset shape after removing outliers:", data_cleaned.shape)

#2: Encoding Categorical Data
encoder = OneHotEncoder(drop='first')
categorical_features = ['Meal_Type', 'Room_Type', 'Booking_Status']
transformers = [('encoder', encoder, categorical_features)]
column_transformer = ColumnTransformer(transformers, remainder='passthrough')
data_encoded = column_transformer.fit_transform(data)

#3: Feature Selection and Data Cleaning
X = df_cleaned.drop[('booking id','no of adults','number of children','number of weekend nights','date of reservation','special requests','average price','market segment type','lead time','type of meal','number of week nights',), axis=1]
y = df_cleaned['booking status']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

selector = SelectKBest(score_func=f_classif, k=5)
X_selected = selector.fit_transform(X_scaled, y)

selected_feature_names = X.columns[selector.get_support()]

#4: Data Splitting
X = data_encoded[:, :-1]
y = data_encoded[:, -1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#5: Model Development and Training
#Logistic Regression model
logistic_model = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean
    ('classifier', LogisticRegression())  # Logistic regression model
])

logistic_model.fit(X_train, y_train)

#6: Model Evaluation
logistic_y_pred = logistic_model.predict(X_test)
logistic_accuracy = accuracy_score(y_test, logistic_y_pred)
logistic_precision = precision_score(y_test, logistic_y_pred)
logistic_recall = recall_score(y_test, logistic_y_pred)
logistic_f1 = f1_score(y_test, logistic_y_pred)
logistic_conf_matrix = confusion_matrix(y_test, logistic_y_pred)

print("\nLogistic Regression Model Evaluation:")
print("Accuracy:", logistic_accuracy)
print("Precision:", logistic_precision)
print("Recall:", logistic_recall)
print("F1 Score:", logistic_f1)
print("Confusion Matrix:")
print(logistic_conf_matrix)